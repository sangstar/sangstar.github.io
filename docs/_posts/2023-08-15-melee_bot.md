---
layout: post
title:  "Fine-tuning Llama 2 7B using Self-Instruct to give you tips in Super Smash Bros Melee"
date:   2023-06-30 10:42
categories: nlp
usemathjax: true
---

<!-- for mathjax support -->
{% if page.usemathjax %}
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
{% endif %}

This is a small project I've worked on to use LoRa to fine-tune a Llama 2 model to give Smash Bros Melee advice. It was trained off of guides from different sources, but mostly from Smashboards, the most popular Super Smash Bros forum.

I fine-tuned a 7 billion parameter version of Llama 2. This is a relatively lightweight LLM, and as such doesn't produce the impressive responses you're used to seeing with GPT-3.5 or GPT-4 or any other larger models like Llama 2's 70B parameter counterpart. There's increasing research being done on ways to enhance the capabilities of more lightweight language models using bountiful and high quality training data, ideally better than that used to train the larger models. Two efforts in this venture have used ShareGPT.com, a repository of user-shared conversations with ChatGPT, and a technique called self-instruct, which uses a teacher model like GPT-4 to create data and filter for quality. I took inspiration from the latter with this model. I'm going to describe my process in this article.

# Gathering and processing data
I didn't want to simply ask GPT-4 to just zero-shot generate Q&As for the model, as it could risk a lot of redundant Q&As and hallucinations without careful and varied prompting. Instead, I scraped character guides from [Smashboards](https://smashboards.com/), the largest Smash Bros. community. The guides were old but formatted nicely so that scraping wasn't too involved of a process. My aim wasn't to create a model that needed to have up-to-date data as I'm not intending on this model to be some polished marketable product -- I just wanted to show what you can do with a bit of money and some prompting. I then decided that I would want to create Q&As from slices of the text, rather than feeding the entire text itself, the reasons being in order to get more varied Q&As (a smaller text slice will leave less room for GPT-4 to make its own questions from summarizing a huge chunk of text, so will be more specialized) and in part because huge prompts are more costly. I did this dividing the text up into "chunks".

# Creating the self-instruct data
Once I had my chunk generator in play, I ideally wanted to, for each character (I included 8 characters only for compute reasons) systematically set a chunk size, and create Q&As for each chunk. I tried increasing the chunk size sequentially from 2 (which amounts to cutting the whole text in half) to 50, but I thought it might be a cooler idea to decide on the chunk size by sampling from a normal distribution. I'd then clip that result between a min and a max chunk size (typically 2 to 50) and convert it into an integer. That's because I found that larger chunks tended to compute faster (less prompts despite more text), and tended to have more varied answers since it had more text to infer from, but still wanted a chance for a larger chunk size. I chose for all characters besides Fox a mean chunk size of 3 with a standard deviation of 3.5, keeping in mind that clipping applies at 2, so it's a sort of clipped Gaussian, where the deviations typically apply upward in chunk size. I chose a mean of 45 for Fox with a standard deviation of 3, and a minimum value of 35 because I had by far the biggest text corpus on him (any Melee player won't be surprised to hear that if any character should have the lengthies guide it would be him) and smaller chunk sizes tended to be out of even GPT-3.5-16k's context length. 

## Prompting
I wanted to add some variation to the prompting, and thought of something like this:
```
starting_prompt="""
You are a helpful assistant that takes user-written guides on playable characters from Super Smash Brothers Melee for the Nintendo Gamecube and creates questions and answers based on the guide.
As you are producing question and answer pairs for competitive players, please try to ensure the questions and answers are not overly simplistic."""

def enrich_prompt(starting_prompt):
    starting_prompt += "\n"
    technical_decider = np.random.choice([0,1])
    if technical_decider == 1:
        print('Question made more technical')
        starting_prompt += "Questions should be as technical as possible.\n"
        
    length_decider = np.random.choice([0,1,2])
    if length_decider == 1:
        print('Answer lengthy')
        starting_prompt += "Answers should be lengthy.\n"
    if length_decider == 0:
        print('Answer concise')
        starting_prompt += "Answers should be concise.\n"
    else:
        starting_prompt += ""
        
    starting_prompt+="""
Do not mention the guide itself in any way. Split the question answer pairs by two newlines.

#Guide#: 
{guide}

#Generated questions and answers#:
    """
    
    return starting_prompt
```

