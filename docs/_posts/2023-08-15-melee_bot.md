---
layout: post
title:  "Fine-tuning Llama 2 7B to give you tips in Super Smash Bros Melee"
date:   2023-06-30 10:42
categories: nlp
usemathjax: true
---

<!-- for mathjax support -->
{% if page.usemathjax %}
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
{% endif %}

This is a small project I've worked on to use LoRa to fine-tune a Llama 2 model to give Smash Bros Melee advice. It was trained off of guides from different sources, but mostly from Smashboards, the most popular Super Smash Bros forum.

I fine-tuned a 7 billion parameter version of Llama 2. This is a relatively lightweight LLM, and as such doesn't produce the impressive responses you're used to seeing with GPT-3.5 or GPT-4 or any other larger models like Llama 2's 40B parameter counterpart. There's increasing research being done on 