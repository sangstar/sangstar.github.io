---
layout: post
title:  "Transformers and ChatGPT"
date:   2023-03-03 11:42
categories: ml
usemathjax: true
---

<!-- for mathjax support -->
{% if page.usemathjax %}
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
{% endif %}

ChatGPT has blown up the world of ML and seems to have transcended all other machine learning innovations since I've entered the field, seemingly heralding a new epoch (no AI pun intended) of artificial intelligence. Ever since it came out in November I've been using it constantly for things work-related and non-work-related. I'm going to attempt to give a detailed look into how ChatGPT works. This will be relatively heavy on concepts so that it can be lighter on substance to where it's not a 90-minute read. Consider this a read for someone passionate in the field and knows a few basic concepts already regarding NLP. 

ChatGPT is an artificially intelligent chatbot developed by OpenAI that deployed last November. It's built on top of GPT-3, which is part of what I like to consider the pantheon of NLP large language models, particularly notorious for its ability to summarize or generate text. GPT stands for Generative Pre-Trained Transformer. 

I'm going to first start by talking broadly about what transformers are, then get into how GPT's output is stochastic and how that produces "smarter" language, then will overview how ChatGPT uses GPT and proximal policy optimization to be *the* living embodient of this decade's AI paradigm. 

# Transformers
A transformer is a complex type of neural network architecture that really has no peer in natural language processing. It's also relatively new, debuting in a paper titled "Attention is All You Need" by Vaswani et al in 2017. It piggybacks off of sequence-to-sequence models (Seq2Seq) that takes a sequence as input and takes a sequence as outputs. Before their invention, the usage of recurrent neural networks (RNNs) were the standard. Transformers immediately seemed to eclipse RNN's in usage due to their improved capacity to capture long-range dependencies in sequences, ability to process tokens in parallel (as opposed to sequentially, which additionally suffered from the risk of vanishing gradients) and are far more interpretable with self-attention. A transformer is shown below:

<p align="center">
  <img width="auto" height="auto" src="/assets/transformer.jpg">
</p>

which for me, when I first saw it, was absolutely befuddling. My hope is that after reading this article, if you go back to this graphic it'll make a bit more sense. Let's dive right and start dealing with the attention units. I will be describing a transformer that is used to generate text. 

## Self-attention and the encoder
Transformers are composed of scaled dot-product attention units. These attention units embed the input document by each token's weighted relevance to other tokens in the sentence. 

For each attention unit, the model learns weights matrices $$W_Q$$, called the query weights matrix, $$W_K$$, the key weights matrix, and $$W_V$$, the value weights matrix. 

Suppose we have a document $$d$$ with $$n$$ tokens that I will describe in terms of their embeddings: $$d = (u_1, u_2, ..., u_n)$$, which we can assume are produced using a pre-trained word vector space like GloVe or something, and want to render . For each token, we produce a query, key and value vector by multiplying the token's embedding with each of their associated weights matrices. 

Therefore, for a position $$i$$, corresponding to a token $$t_i$$, we have a query vector $$q_i = u_i W_Q$$, a key vector $$k_i = u_i W_K$$ and a value vector $$v_i = u_i W_V$$. Then, for each position $$i$$, we get an attention weight between positions $$i$$ and $$j$$ by taking the dot product of $$q_i$$ with $$k_j$$:

$$\alpha_{i,j} = \frac{q_i^T k_j}{\sqrt{n}}$$

which can be thought of as the relevance of token $$i$$ on token $$j$$. These attention weight are computed for all $$n$$ tokens, so that for token $$i$$ you have a vector of attention units $$(\alpha_{i,1}, \alpha_{i,2}, ... , \alpha_{i,n})$$ which includes $$\alpha_{i,i}$$, for completeness sake. The vector is softmaxxed so that all the scalars add up to unity, and to denote that I will rewrite the attention units as $$(a_{i,1}, a_{i,2}, ... , a_{i,n})$$. This vector then acts as weight for the weighted sum of value vectors for each position:

$$\text{Attention}_i = \sum_{j=1}^n a{i,j} v_j$$

where $$a_i$$ is known as the *attention* for token or position $$i$$. By doing this for all tokens, you've *encoded* your document $$d$$. Notice that through all these vectorized operations we've managed to enable full-range context dependency on all of our tokens, which RNN's aren't able to do. In addition, this setup allows for asymmetry between tokens $$i$$ and $$j$$. As $$\alpha_{i,j} \ne \alpha_{j,i}$$, this means that the influence of token $$i$$ on $$j$$ can be large while the influence of token $$j$$ on $$i$$ can be mild and vice versa. 

One set of weights matrices each is referred to as an *attention head*, and each attention layer will have multiple heads, as you can see from the "multi-head attention" stuff in the above image. The reason why you want to use multiple heads is similar to why you want to use multiple hidden layers in a neural net, in that it allows more insights to be picked up by combining the "ideas" of all the heads. 

The final multi-headed attention is a concatenation of all of the attention vectors multiplied by a final weights matrix $$W_O$$:

$$\text{MultiheadAttention(Q,K,V)} = \text{Concat}(\text{Attention}_i)W_O$$

which is fed to a feed-forward neural network as essentially an "enhanced" and "wiser" version of a word embedding, usually then passed to an $$\text{Add&Norm}$$ layer which adds the output of the feed-forward layer to the output of the multiheaded attention layer which helps to stabilize the training process by reducing the effect of the variance in the distributions of the intermediate activations of the self-attention and feed-forward layers which helps remedy the risk of vanishing gradients. 

This is known as an encoder. There usually are a number of these in popular transformers (BERT and GPT-2 range usually between 12 and 24). The first encoder takes word embeddings as input, while the subsequent encoders take encodings. 

I'm going to skip positional encoding other than to say it provides a way to capture the order of an input sequence, which is good because otherwise it couldn't distinguish between two tokens that have the same value vector but appear at different positions in the input sequence.


## Decoders
The decoder is responsible for actually producing the output. For text generation, it takes as input the previously generated output tokens in the output, and the output of the encoding layers. If no tokens have been generated so far it starts with a start-of-sequence token $$\text{SOS}$$. 

Firstly, self-attention is applied to the output tokens. In order to ensure the decoder doesn't "cheat" and use current or future output tokens in its autoregressive generation, it uses a attention layer that *masks* tokens ahead of token $$t$$ before the softmax stage. Then, the encoded input and previously generated tokens are all used as input for another multi-head attention layer, which allows it to focus on relevant parts of the input sequence to generate the next token. Another way of saying this is that the decoder *attends to* the encoded input as well as the previously generated tokens

This net encoding of the encoded input with the previously generated tokens is finally used as input for a feed-forward neural network, which produces a softmaxxed probability distribution of likely next tokens, and this process iterates until you hit a max token length or an end-of-sequence token $$\text{EOS}$$. 

Yes, that's pretty much the gist of it. It's complicated, but if you were to take anything away, it would be this:

1. Transformers leverage context between tokens in each sequence, not just a token's absolute sentimental meaning, all without a dependency range limit. 
2. Text-generation transformers leverage context between tokens in an input sequence to tokens in their output sequence, as well as the output sequence *with itself*. 



## References

