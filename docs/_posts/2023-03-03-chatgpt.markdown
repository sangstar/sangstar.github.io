---
layout: post
title:  "Transformers and ChatGPT"
date:   2023-02-25 8:58
categories: ml
usemathjax: true
---

<!-- for mathjax support -->
{% if page.usemathjax %}
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
{% endif %}

ChatGPT has blown up the world of ML and seems to have transcended all other machine learning innovations since I've entered the field, seemingly heralding a new epoch (no AI pun intended) of artificial intelligence. Ever since it came out in November I've been using it constantly for things work-related and non-work-related. I'm going to attempt to give a detailed look into how ChatGPT works. This will be relatively heavy on concepts so that it can be lighter on substance to where it's not a 90-minute read. Consider this a read for someone passionate in the field and knows a few basic concepts already regarding NLP. 

ChatGPT is an artificially intelligent chatbot developed by OpenAI that deployed last November. It's built on top of GPT-3, which is part of what I like to consider the pantheon of NLP large language models, particularly notorious for its ability to summarize or generate text. GPT stands for Generative Pre-Trained Transformer. 

I'm going to first start by talking about what transformers are, then get into how GPT's output is stochastic and how that produces "smarter" language, then will overview how ChatGPT uses GPT and proximal policy optimization to be *the* living embodient of this decade's AI paradigm. 

# Transformers
A transformer is a complex type of neural network architecture that really has no peer in natural language processing. It's also relatively new, debuting in a paper titled "Attention is All You Need" by Vaswani et al in 2017. It piggybacks off of sequence-to-sequence models (Seq2Seq) that takes a sequence as input and takes a sequence as outputs. Before their invention, the usage of recurrent neural networks (RNNs) were the standard. Transformers immediately seemed to eclipse RNN's in usage due to their improved capacity to capture long-range dependencies in sequences, ability to process tokens in parallel (as opposed to sequentially, which additionally suffered from the risk of vanishing gradients) and are far more interpretable with self-attention. A transformer is shown below:

<p align="center">
  <img width="auto" height="auto" src="/assets/transformer.jpg">
</p>

which for me, when I first saw it, was absolutely befuddling. My hope is that after reading this article, if you go back to this graphic it'll make a bit more sense. Let's dive right and start dealing with the attention units. 

## Self-attention
Transformers are composed of scaled dot-product attention units. These attention units embed the input document by each token's weighted relevance to other tokens in the sentence. 

For each attention unit, the model learns weights matrices $$W_Q$$, called the query weights matrix, $$W_K$$, the key weights matrix, and $$W_V$$, the value weights matrix. 

Suppose we have a document $$d$$ with $$n$$ tokens that I will describe in terms of their embeddings: $$d = (u_1, u_2, ..., u_n)$$, which we can assume are produced using a pre-trained word vector space like GloVe or something, and want to render . For each token, we produce a query, key and value vector by multiplying the token's embedding with each of their associated weights matrices. 

Therefore, for a position $$i$$, corresponding to a token $$t_i$$, we have a query vector $$q_i = u_i W_Q$$, a key vector $$k_i = u_i W_K$$ and a value vector $$v_i = u_i W_V$$. Then, for each position $$i$$, we get an attention weight between positions $$i$$ and $$j$$ by taking the dot product of $$q_i$$ with $$k_j$$:

$$\alpha_{i,j} = \frac{q_i^T k_j}{\sqrt{n}}$$

which can be thought of as the relevance of token $$i$$ on token $$j$$. These attention weight are computed for all $$n$$ tokens, so that for token $$i$$ you have a vector of attention units $$(\alpha_{i,1}, \alpha_{i,2}, ... , \alpha_{i,n})$$ which includes $$\alpha_{i,i}$$, for completeness sake. The vector is softmaxxed so that all the scalars add up to unity, and to denote that I will rewrite the attention units as $$(\a_{i,1}, \a{i,2}, ... , \a{i,n})$$. This vector then acts as weight for the weighted sum of value vectors for each position:

$$\text{Attention}_i = \sum_{j=1}^n \a{i,j} v_j$$

where $$a_i$$ is known as the *attention* for token or position $$i$$. By doing this for all tokens, you've *encoded* your document $$d$$. Notice that through all these vectorized operations we've managed to enable full-range context dependency on all of our tokens, which RNN's aren't able to do. In addition, this setup allows for asymmetry between tokens $$i$$ and $$j$$. As $$\alpha_{i,j} \ne \alpha{j,i}$$, this means that the influence of token $$i$$ on $$j$$ can be large while the influence of token $$j$$ on $$i$$ can be mild. 



## References

